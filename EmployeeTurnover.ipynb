{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definição do Problema\n",
    "\n",
    "Prever o turnover (rotatividade) de empregados com base em várias características, como demografia, profissão, características psicológicas e outros atributos relacionados ao trabalho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "turnover_data = pd.read_csv('turnover.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visão Geral dos Dados\n",
    "\n",
    "Ao visualizar os dados vemos que existem 16 atributos, dos quais \"EVENT\" sera o atributo target da nossa predição.\n",
    "\n",
    "**stag**: Tempo t quando ocorreu o turnover ou tempo de censura do estudo.\n",
    "**event**: Indica se o evento ocorreu no tempo t.  \n",
    "**gender**: Gênero do funcionário.  \n",
    "**age**: Idade do funcionário.  \n",
    "**industry**: Indústria de atuação do funcionário.  \n",
    "**profession**: Profissão do funcionário.  \n",
    "**traffic**: De qual canal o funcionário ingressou para a empresa  \n",
    "**coach**: Presença de um coach  \n",
    "**head_gender**: Gênero do supervisor ao qual o funcionario responde  \n",
    "**greywage**: se o empregador paga apenas uma pequena quantia de salário acima do salário mínimo (white).  \n",
    "**way**: Forma que o funcionário se desloca para o escritório.  \n",
    "**extraversion**: Escala segundo o teste Big5.  \n",
    "**independ**: Escala segundo o teste Big5.  \n",
    "**selfcontrol**: Escala segundo o teste Big5.  \n",
    "**anxiety**: Escala segundo o teste Big5.  \n",
    "**novator**: Escala segundo o teste Big5.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turnover_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-processamento de Dados\n",
    "\n",
    "## Tratamento de Nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    'Null Count': turnover_data.isnull().sum().fillna(0).astype(int),\n",
    "    'Dtype': turnover_data.dtypes\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em uma primeira análise vemos que nao existem valores nulos na nossa base, assim como os tipos de valores existentes são:\n",
    "\n",
    "quantitativo continuo = 7  \n",
    "qualitativo nominal = 8\n",
    "\n",
    "Tambem vemos que na base nao existem valores nulos ou faltando"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valores repetidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_rows = turnover_data[turnover_data.duplicated()]\n",
    "print(f\"Numero de linhas duplicadas = {duplicated_rows.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turnover_data = turnover_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para determinar os outliers é necessário fazer um estudo sobre os quartis e ver como estes se comportam, porque assim é possivel saber se existe uma tendencia a possuir mais valores de um tipo do que outro e assim analisar os outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turnover_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Devido à natureza de que outliers so irão existir em categorias numéricas, diversas colunas podem ser ignoradas. Além disso, com exceção de stag, todas as colunas apresentam uma distribuição igualitária onde o valor do quartil se encaixa de maneira consideravelmente próxima à porcentagem do valor máximo em relação ao mínimo. Portanto a coluna stag é a unica a qual deve ser analisado a existência de outliers.\n",
    "\n",
    "Nesta situação uma boa técnica é o uso do z-score, o qual mede o desvio padrão de uma instância em relação à media, assim podemos captar outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo dos z-scores para todas as colunas numéricas do DataFrame\n",
    "z_scores = (turnover_data['stag'] - turnover_data['stag'].mean()) / turnover_data['stag'].std()\n",
    "\n",
    "# Recuperando as linhas que possuem outliers baseado no z-score 3\n",
    "outliers = z_scores[(z_scores > 3) | (z_scores < -3)].index\n",
    "\n",
    "len(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turnover_data = turnover_data.drop(outliers)\n",
    "turnover_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O outlier 2.5 se mostrou adequado pois foi o que mais aproximou o valor discrepante do padrão de quartis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanceamento\n",
    "\n",
    "Para saber se uma base de dados é balanceada deve-se olhar a destribuição entre o atributo target, que neste caso é a coluna **event**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Análise da distribuição da variável alvo\n",
    "event_distribution = turnover_data['event'].value_counts()\n",
    "\n",
    "# Plotando a distribuição\n",
    "plt.figure(figsize=(8, 6))\n",
    "event_distribution.plot(kind='bar', color=['skyblue', 'salmon'])\n",
    "plt.title('Distribuição da Variável Alvo (event)')\n",
    "plt.xlabel('Classe')\n",
    "plt.ylabel('Quantidade')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n",
    "\n",
    "event_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que a base nao é desbalanceada, basta seguir em frente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformação de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "# Seleciona apenas colunas não numéricas\n",
    "non_numeric_columns = turnover_data.select_dtypes(exclude=['float64', 'int64'])\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "# Para cada coluna categórica\n",
    "for col in non_numeric_columns.columns:\n",
    "    values = turnover_data[col].value_counts().tolist()\n",
    "    plt.plot(range(len(values)), values, marker='.', label=col)\n",
    "\n",
    "plt.xlabel('Índice dos Atributos')\n",
    "plt.ylabel('Quantidade')\n",
    "plt.title('Distribuição de atributos categoricos')\n",
    "plt.legend(title='Colunas Categóricas', loc='upper right')\n",
    "plt.grid(True, which='both', linestyle=':', linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisando a quantidade de possíveis instâncias por coluna chega-se a conclusão de que existem:  \n",
    "3 colunas binarizáveis (gender, head_gender e greywage)  \n",
    "2 colunas pequenas o suficiente para realizar encode ou one hot (coach e way)  \n",
    "3 colunas que precisarão de um estudo de caso para decidir como tratá-las  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise da variável 'industry'\n",
    "industry_distribution = turnover_data['industry'].value_counts(normalize=True)\n",
    "\n",
    "# Plotando a distribuição\n",
    "plt.figure(figsize=(6, 6))\n",
    "industry_distribution.plot(kind='barh', color='cornflowerblue')\n",
    "plt.title('Distribuição da Variável \"industry\"')\n",
    "plt.xlabel('Proporção')\n",
    "plt.ylabel('Indústria')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Industry possui diversos valores com uma destribuição considerável em cada feature.  \n",
    "Por isso, a estratégia adotada será realizar um encoding onde o valor de industry será substituído pela porcentagem de quantos desses resultados terminaram em turnover, assim ainda permitindo que o modelo retire insights valiosos da feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculando a relação de target (mean encoding) para a variável 'industry'\n",
    "industry_target_relation = turnover_data.groupby('industry')['event'].mean()\n",
    "\n",
    "# Ordenando os valores para melhor visualização\n",
    "industry_target_relation_sorted = industry_target_relation.sort_values(ascending=False)\n",
    "\n",
    "# Plotando a relação de target da variável 'industry'\n",
    "plt.figure(figsize=(6, 6))\n",
    "industry_target_relation_sorted.plot(kind='barh', color='lightseagreen')\n",
    "plt.title('Relação de Target da Variável \"industry\"')\n",
    "plt.xlabel('Média de Turnover (Event)')\n",
    "plt.ylabel('Indústria')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado uma relação satisfatória que separa os atributos, o tornam válido para a utilização da técnica, portanto pode-se substituir na tabela original o novo valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando a codificação target (mean encoding) para a variável 'industry' no dataset original\n",
    "turnover_data['industry_encoded'] = turnover_data['industry'].map(industry_target_relation)\n",
    "\n",
    "# Substituindo a coluna 'industry' pela coluna 'industry_encoded'\n",
    "turnover_data.drop('industry', axis=1, inplace=True)\n",
    "turnover_data.rename(columns={'industry_encoded': 'industry'}, inplace=True)\n",
    "\n",
    "# Exibindo as primeiras linhas do dataframe após a substituição\n",
    "turnover_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise da variável 'profession'\n",
    "profession_distribution = turnover_data['profession'].value_counts(normalize=True)\n",
    "\n",
    "# Plotando a distribuição\n",
    "plt.figure(figsize=(6, 6))\n",
    "profession_distribution.plot(kind='barh', color='lightgreen')\n",
    "plt.title('Distribuição da Variável \"profession\"')\n",
    "plt.xlabel('Proporção')\n",
    "plt.ylabel('Profissão')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diferente de Industry, Profession possui uma grande quantidade de instâncias aglomeradas em uma única classe de maneira desbalanceada, portanto os valores de classes como PR não irão apresentar um resultado relevante para o nosso modelo. Para tratar isso as classes com uma quantidade de instâncias menor que \"Etc\" são colocadas dentro desta mesma classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculando a distribuição das categorias em 'profession'\n",
    "profession_distribution_original = turnover_data['profession'].value_counts(normalize=True)\n",
    "\n",
    "# Identificando as categorias que serão agrupadas em \"etc\"\n",
    "threshold_original = profession_distribution_original['etc']\n",
    "categories_to_group_original = profession_distribution_original[profession_distribution_original <= threshold_original].index\n",
    "\n",
    "# Agrupando categorias menos frequentes em \"etc\"\n",
    "turnover_data['profession_grouped'] = turnover_data['profession'].apply(lambda x: 'etc' if x in categories_to_group_original else x)\n",
    "\n",
    "# Verificando a distribuição após o agrupamento\n",
    "updated_profession_distribution = turnover_data['profession_grouped'].value_counts(normalize=True)\n",
    "\n",
    "# Plotando a distribuição da variável 'profession_grouped'\n",
    "plt.figure(figsize=(6, 6))\n",
    "updated_profession_distribution.plot(kind='bar', color='lightcoral')\n",
    "plt.title('Distribuição da Variável \"profession_grouped\"')\n",
    "plt.xlabel('Profissão Agrupada')\n",
    "plt.ylabel('Proporção')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estando as instâncias categorizadas em uma pequena quantidade de classes, é possível utilizar One Hot Encoding para classificar o atributo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizando one-hot encoding para 'profession_grouped'\n",
    "profession_onehot = pd.get_dummies(turnover_data['profession_grouped'], prefix='profession', drop_first=False)\n",
    "\n",
    "# Concatenando o one-hot encoding ao dataframe original\n",
    "turnover_data = pd.concat([turnover_data, profession_onehot], axis=1)\n",
    "\n",
    "# Removendo as colunas 'profession' e 'profession_grouped' já que não são mais necessárias\n",
    "turnover_data.drop(['profession', 'profession_grouped'], axis=1, inplace=True)\n",
    "\n",
    "# Exibindo as primeiras linhas do dataframe atualizado\n",
    "turnover_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise da variável 'traffic'\n",
    "traffic_distribution = turnover_data['traffic'].value_counts(normalize=True)\n",
    "\n",
    "# Plotando a distribuição\n",
    "plt.figure(figsize=(6, 6))\n",
    "traffic_distribution.plot(kind='barh', color='orchid')\n",
    "plt.title('Distribuição da Variável \"traffic\"')\n",
    "plt.xlabel('Proporção')\n",
    "plt.ylabel('Fonte de Recrutamento')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traffic acaba tendo o mesmo problema de Industry por não ter um desbalanceamento demasiadamente grande, por isso é sugerido seguir pelo mesmo caminho de realizar um mean encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculando mean encoding para a variável 'traffic'\n",
    "traffic_mean_encoding = turnover_data.groupby('traffic')['event'].mean()\n",
    "\n",
    "# Aplicando a codificação para visualização\n",
    "turnover_data['traffic_encoded'] = turnover_data['traffic'].map(traffic_mean_encoding)\n",
    "\n",
    "# Exibindo as primeiras linhas da coluna original 'traffic' e da coluna codificada 'traffic_encoded'\n",
    "traffic_comparison = turnover_data[['traffic', 'traffic_encoded']].drop_duplicates().sort_values(by='traffic_encoded', ascending=False)\n",
    "\n",
    "# Substituindo a coluna 'traffic' pela coluna 'traffic_encoded'\n",
    "turnover_data.drop('traffic', axis=1, inplace=True)\n",
    "turnover_data.rename(columns={'traffic_encoded': 'traffic'}, inplace=True)\n",
    "\n",
    "# Exibindo as primeiras linhas do dataframe após a substituição\n",
    "turnover_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coach pode ser considerado um atributo binário a partir de certo ponto de vista, mas para a pesquisa foi diferenciado entre o coach ser um agente externo ou o próprio treinador do funcionàrio, para isso é seguro afirmar que podemos realizar um encoding contando que \"my head\" seja colocado ao lado de \"yes\" como um supertipo, mas não importando a ordem entre os dois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculando a distribuição dos valores na coluna 'coach'\n",
    "coach_distribution = turnover_data['coach'].value_counts(normalize=True)\n",
    "\n",
    "# Plotando a distribuição dos valores\n",
    "plt.figure(figsize=(6, 6))\n",
    "coach_distribution.plot(kind='bar', color='lightgreen')\n",
    "plt.title('Coach distribution')\n",
    "plt.xlabel('Coach')\n",
    "plt.ylabel('Proporção')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n",
    "\n",
    "coach_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a coluna 'coach_encoded'\n",
    "turnover_data['coach_encoded'] = turnover_data['coach'].map({'yes': 2, 'no': 0, 'my head': 1})\n",
    "\n",
    "# Removendo a coluna original 'coach'\n",
    "turnover_data.drop('coach', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os valores de way não são possíveis de se relacionar diretamente por não possuirem uma cardinalidade, portanto deve-se usar uma estratégia de separá-los em atributos binarizados, já que possuem apenas 3 tipos de instâncias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculando a distribuição dos valores na coluna 'way'\n",
    "coach_distribution = turnover_data['way'].value_counts(normalize=True)\n",
    "\n",
    "# Plotando a distribuição dos valores\n",
    "plt.figure(figsize=(6, 6))\n",
    "coach_distribution.plot(kind='bar', color='lightgreen')\n",
    "plt.title('Way distribution')\n",
    "plt.xlabel('Way')\n",
    "plt.ylabel('Proporção')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n",
    "\n",
    "coach_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizando one-hot encoding para a variável 'way'\n",
    "way_onehot = pd.get_dummies(turnover_data['way'], prefix='way', drop_first=False)\n",
    "\n",
    "# Concatenando o one-hot encoding ao dataframe original\n",
    "turnover_data = pd.concat([turnover_data, way_onehot], axis=1)\n",
    "\n",
    "# Removendo a coluna original 'way' já que não é mais necessária\n",
    "turnover_data.drop('way', axis=1, inplace=True)\n",
    "\n",
    "# Exibindo as primeiras linhas do dataframe após o one-hot encoding\n",
    "turnover_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarização de atributos com 2 categorias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificando colunas com exatamente 2 valores únicos\n",
    "binary_columns = [col for col in turnover_data.columns if turnover_data[col].nunique() == 2]\n",
    "\n",
    "# Binarizando as colunas identificadas\n",
    "for col in binary_columns:\n",
    "    unique_vals = turnover_data[col].unique()\n",
    "    turnover_data[col] = turnover_data[col].replace({unique_vals[0]: 0, unique_vals[1]: 1})\n",
    "\n",
    "# Exibindo as primeiras linhas do dataframe após a binarização\n",
    "turnover_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalização \\ Padronização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibindo as primeiras linhas do dataframe após a normalização\n",
    "turnover_data.describe().loc[['min', 'max']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Devido aos valores estarem contidos dentro de um range relativamente curto, não se torna obrigatório realizar a normalização, mas ainda assim se torna benéfico para a qualidade do nosso modelo para evitar peso desnecessário em stag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificando colunas que precisam ser normalizadas\n",
    "columns_to_normalize = [col for col in turnover_data.columns \n",
    "                        if turnover_data[col].min() < 0 or turnover_data[col].max() > 1]\n",
    "\n",
    "# Normalizando as colunas identificadas\n",
    "for col in columns_to_normalize:\n",
    "    turnover_data[col] = (turnover_data[col] - turnover_data[col].min()) / \\\n",
    "                                 (turnover_data[col].max() - turnover_data[col].min())\n",
    "\n",
    "turnover_data.describe().loc[['min', 'max']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise Exploratória de Dados\n",
    "\n",
    "Atraves da analise de correlação entre os elementos podemos ver se o crescimento das variaveis segue direta ou inversamente o crescimento de determinada variavel alvo, seguindo uma escala que começa de 1 se for diretamente proporcional ou até -1 se for inversamente proporcional.\n",
    "Nem sempre estas relações significam que no treinamento do modelo elas não possuirão relevancia, mas ainda sim é um grande indicativo e muitos insights podem ser retirados delas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Calculando a matriz de correlação\n",
    "correlation_matrix = turnover_data.corr()\n",
    "\n",
    "# Plotando a matriz de correlação usando um heatmap\n",
    "plt.figure(figsize=(20, 15))\n",
    "ax = sns.heatmap(correlation_matrix, cmap='RdBu_r', fmt='.2f', linewidths=0.5)\n",
    "\n",
    "# Adicionar anotações numéricas manualmente\n",
    "for i in range(correlation_matrix.shape[0]):\n",
    "    for j in range(correlation_matrix.shape[1]):\n",
    "        value = correlation_matrix.iloc[i, j]\n",
    "        # Ajustando a cor baseada no valor da correlação\n",
    "        text_color = 'black' if -0.5 < value < 0.5 else 'white'\n",
    "        ax.text(j + 0.5, i + 0.5, f'{value:.2f}', ha='center', va='center', color=text_color, fontsize=14)\n",
    "\n",
    "plt.title('Matriz de Correlação')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E possível identificar que existe correlação consideravelmente alta entre os valores de way_car para way_bus assim como de professional_etc para professional_HR\n",
    "\n",
    "mas para o caso de way_car, a relação entre este valor para way_foot fornece um insight importante para perceber que por mesmo que exista uma correlação próxima, ele ainda é um forte candidato a realizar desempates em relação a terceiras e quartas variáveis.\n",
    "\n",
    "Ambos os dois casos naturalmente possuem uma correlação devido ao fato de serem mutuamente excludentes durante seu processo de encoding, mas ainda assim seus valores interferem na avaliação final do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agora correlacionando exclusivamente os valores em relação ao evento em questão temos os seguintes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turnover_data.corr()['event'].sort_values(ascending=False).drop('event')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Levando isso em conta e testes que foram realizados no modelo final, concluimos que gender, profession_IT, novator e coach_encoded diminuem o score geral dos modelos. Isso juntamente com sua pontuação proxima de 0 na analise de correlação nos faz decidir tira-los da tabela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo as variáveis relacionadas a 'profession', 'way' e 'greywage'\n",
    "columns_to_drop = ['gender'] + ['profession_IT'] + ['novator'] + ['coach_encoded']\n",
    "turnover_data = turnover_data.drop(columns=columns_to_drop)\n",
    "\n",
    "# Verificando as primeiras linhas do dataframe após a remoção das variáveis\n",
    "turnover_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divisão dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como a validação do modelo sera feito através da utilização de cross validation então apenas separaremos o dados entre X e y\n",
    "A separação exata em folds sera realizada automaticamente durante a utilização do grid-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = turnover_data.drop('event', axis=1)\n",
    "y = turnover_data['event']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criação dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Criando os modelos\n",
    "dt = DecisionTreeClassifier()\n",
    "rf = RandomForestClassifier()\n",
    "nb = GaussianNB()\n",
    "gb = GradientBoostingClassifier()\n",
    "lr = LogisticRegression()\n",
    "ab = AdaBoostClassifier()\n",
    "kn = KNeighborsClassifier()\n",
    "sv = SVC()\n",
    "nn = MLPClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição de hiperparâmetros para o uso do Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "param_dist_dt = {\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10, 15, 20],\n",
    "    'min_samples_leaf': [1, 2, 4, 6, 8],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Random Forest\n",
    "param_dist_rf = {\n",
    "    'n_estimators': [50, 100, 150, 200, 250],\n",
    "    'max_depth': [None, 10, 20, 30, 40],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Naive Bayes\n",
    "param_dist_nb = {\n",
    "    'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]\n",
    "}\n",
    "\n",
    "# Gradient Boosting\n",
    "param_dist_gb = {\n",
    "    'loss': ['log_loss', 'exponential'],\n",
    "    'learning_rate': [0.001, 0.1, 1],\n",
    "    'n_estimators': [50, 100, 200, 500],\n",
    "    'criterion': ['friedman_mse', 'squared_error']\n",
    "}\n",
    "\n",
    "# Regressão Logistica\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "param_dist_lr = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'max_iter': [50, 100, 500, 1000, 5000],\n",
    "    'solver': ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'],\n",
    "    \n",
    "}\n",
    "\n",
    "# AdaBoost\n",
    "param_dist_ab = {\n",
    "    'n_estimators': [50, 100, 200, 500],\n",
    "    'learning_rate': [0.001, 0.01, 0.1, 1],\n",
    "    'algorithm': ['SAMME', 'SAMME.R']\n",
    "}\n",
    "\n",
    "# KNeighbors\n",
    "param_dist_kn = {\n",
    "    'n_neighbors': [3, 5, 8],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': [20, 30, 50],\n",
    "    'p': [1, 2]\n",
    "}\n",
    "\n",
    "param_dist_sv = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'kernel': ['poly', 'sigmoid'],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'coef0': [0.0, 0.5, 1.0],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "}\n",
    "\n",
    "param_dist_nn = {\n",
    "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento dos modelos utilizando o Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definindo a instancia de Grid Search\n",
    "grid_search_dt = GridSearchCV(dt, param_grid=param_dist_dt, cv=10, n_jobs=1, verbose = 1, error_score='raise')\n",
    "grid_search_rf = GridSearchCV(rf, param_grid=param_dist_rf, cv=10, n_jobs=1, verbose = 1, error_score='raise')\n",
    "grid_search_gb = GridSearchCV(gb, param_grid=param_dist_gb, cv=10, n_jobs=1, verbose = 1, error_score='raise')\n",
    "grid_search_ab = GridSearchCV(ab, param_grid=param_dist_ab, cv=10, n_jobs=1, verbose = 1, error_score='raise')\n",
    "grid_search_kn = GridSearchCV(kn, param_grid=param_dist_kn, cv=10, n_jobs=1, verbose = 1, error_score='raise')\n",
    "grid_search_nb = GridSearchCV(nb, param_grid=param_dist_nb, cv=10, n_jobs=1, verbose = 1, error_score='raise')\n",
    "grid_search_lr = GridSearchCV(lr, param_grid=param_dist_lr, cv=10, n_jobs=1, verbose = 1, error_score='raise')\n",
    "grid_search_sv = GridSearchCV(sv, param_grid=param_dist_sv, cv=10, n_jobs=1, verbose = 1, error_score='raise')\n",
    "grid_search_nn = GridSearchCV(nn, param_grid=param_dist_nn, cv=10, n_jobs=1, verbose = 1, error_score='raise')\n",
    "\n",
    "# Ajustando os valores\n",
    "print('------------ Decision Tree ------------')\n",
    "grid_search_dt.fit(X, y)\n",
    "print('------------ Random Forest ------------')\n",
    "grid_search_rf.fit(X, y)\n",
    "print('---------- Gradient Boosting ----------')\n",
    "grid_search_gb.fit(X, y)\n",
    "print('-------------- Ada Boost --------------')\n",
    "grid_search_ab.fit(X, y)\n",
    "print('------------- K Neighbors -------------')\n",
    "grid_search_kn.fit(np.ascontiguousarray(X), np.ascontiguousarray(y))\n",
    "print('------------- Naive Bayes -------------')\n",
    "grid_search_nb.fit(X, y)\n",
    "print('--------- Regressão Logistica ---------')\n",
    "grid_search_lr.fit(X, y)\n",
    "print('-------- Support Vector Machine -------')\n",
    "grid_search_sv.fit(X, y)\n",
    "print('---------- Neural Network MLP ---------')\n",
    "grid_search_nn.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Melhores valores encontrados no Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Decision Tree          = {grid_search_dt.best_params_}')\n",
    "print(f'Random Forest          = {grid_search_rf.best_params_}')\n",
    "print(f'Gradient Boosting      = {grid_search_gb.best_params_}')\n",
    "print(f'Ada Boost              = {grid_search_ab.best_params_}')\n",
    "print(f'K-Neighbors            = {grid_search_kn.best_params_}')\n",
    "print(f'Naive Bayes            = {grid_search_nb.best_params_}')\n",
    "print(f'Regressão Logistica    = {grid_search_lr.best_params_}')\n",
    "print(f'Support Vector Machine = {grid_search_sv.best_params_}')\n",
    "print(f'Neural Network MLP     = {grid_search_nn.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validação com os algoritmos devidamente treinados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Retirando as previsões encontradas no cross-validation do melhor grid search\n",
    "dt_y_pred_cv = cross_val_predict(grid_search_dt.best_estimator_, X, y, cv=10)\n",
    "rf_y_pred_cv = cross_val_predict(grid_search_rf.best_estimator_, X, y, cv=10)\n",
    "gb_y_pred_cv = cross_val_predict(grid_search_gb.best_estimator_, X, y, cv=10)\n",
    "ab_y_pred_cv = cross_val_predict(grid_search_ab.best_estimator_, X, y, cv=10)\n",
    "kn_y_pred_cv = cross_val_predict(grid_search_kn.best_estimator_, np.ascontiguousarray(X), np.ascontiguousarray(y), cv=10)\n",
    "nb_y_pred_cv = cross_val_predict(grid_search_nb.best_estimator_, X, y, cv=10)\n",
    "lr_y_pred_cv = cross_val_predict(grid_search_lr.best_estimator_, X, y, cv=10)\n",
    "sv_y_pred_cv = cross_val_predict(grid_search_sv.best_estimator_, X, y, cv=10)\n",
    "nn_y_pred_cv = cross_val_predict(grid_search_nn.best_estimator_, X, y, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retirada do score encontrado nos modelos com cross validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "modelos = [\n",
    "    ('DT', grid_search_dt.best_estimator_),\n",
    "    ('RF', grid_search_rf.best_estimator_),\n",
    "    ('GB', grid_search_gb.best_estimator_),\n",
    "    ('AB', grid_search_ab.best_estimator_),\n",
    "    ('KNN', grid_search_kn.best_estimator_),\n",
    "    ('NB', grid_search_nb.best_estimator_),\n",
    "    ('LR', grid_search_lr.best_estimator_),\n",
    "    ('SVM', grid_search_sv.best_estimator_),\n",
    "    ('MLP', grid_search_nn.best_estimator_),\n",
    "]\n",
    "\n",
    "acuracias_modelos = {i: cross_val_score(modelo, np.ascontiguousarray(X), y, cv=5, scoring='accuracy') for i, modelo in modelos}\n",
    "acuracias_modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matriz de confusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "conf_matrix_dt = confusion_matrix(y, dt_y_pred_cv)\n",
    "conf_matrix_rf = confusion_matrix(y, rf_y_pred_cv)\n",
    "conf_matrix_gb = confusion_matrix(y, gb_y_pred_cv)\n",
    "conf_matrix_ab = confusion_matrix(y, ab_y_pred_cv)\n",
    "conf_matrix_kn = confusion_matrix(y, kn_y_pred_cv)\n",
    "conf_matrix_nb = confusion_matrix(y, nb_y_pred_cv)\n",
    "conf_matrix_lr = confusion_matrix(y, lr_y_pred_cv)\n",
    "conf_matrix_sv = confusion_matrix(y, sv_y_pred_cv)\n",
    "conf_matrix_nn = confusion_matrix(y, nn_y_pred_cv)\n",
    "\n",
    "matrices = [conf_matrix_dt, conf_matrix_rf, conf_matrix_gb, conf_matrix_ab, conf_matrix_kn, conf_matrix_nb, conf_matrix_lr, conf_matrix_sv, conf_matrix_nn]\n",
    "titles = ['Decision Tree', 'Random Forest', 'Gradient Boosting', 'Ada Boost', 'K-Nearest', 'Naive Bayes', 'Logistic Regression', 'SVM', 'Neural Network']\n",
    "\n",
    "# 3 linhas, 3 colunas\n",
    "fig, axarr = plt.subplots(3, 3, figsize=(6, 7.5))  \n",
    "\n",
    "# Para facilitar a iteração, vamos achatá-lo (flatten)\n",
    "axes = axarr.flatten()\n",
    "\n",
    "classes = ['No', 'Yes']  # Classes\n",
    "\n",
    "# Agora iteramos sobre as matrizes e os títulos como antes, mas usando o axes achatado\n",
    "for i, (matrix, title) in enumerate(zip(matrices, titles)):\n",
    "    axes[i].imshow(matrix, cmap=\"Blues\", aspect='auto')\n",
    "    axes[i].set_title(title, fontsize=14)\n",
    "    \n",
    "    # Adicionar anotações numéricas com tamanho de fonte maior\n",
    "    for j in range(matrix.shape[0]):\n",
    "        for k in range(matrix.shape[1]):\n",
    "            axes[i].text(k, j, str(matrix[j, k]), ha='center', va='center', fontsize=16)\n",
    "    \n",
    "    axes[i].axis('off')\n",
    "    rect = Rectangle((0,0), 1, 1, edgecolor='black', facecolor='none', transform=axes[i].transAxes, linewidth=3)\n",
    "    axes[i].add_patch(rect)\n",
    "\n",
    "    # Rótulos das linhas (classe verdadeira)\n",
    "    if i == 0 or i == 3 or i == 6:\n",
    "        for j, cls in enumerate(classes):\n",
    "            axes[i].text(-0.6, j, cls, ha='right', va='center', color='black', transform=axes[i].transData)\n",
    "        # Rótulos das colunas (classe prevista)\n",
    "    for j, cls in enumerate(classes):\n",
    "        axes[i].text(j, 1.9, cls, ha='center', va='bottom', color='black', transform=axes[i].transData)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de uma visão geral da matriz de confusão conseguimos perceber que existe uma proporção entre a predição correta de valores sim/nao, mostrando que a precisão irá se aproximar bastante do recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validação do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, model_name):\n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': round(accuracy_score(y_true, y_pred), 3),\n",
    "        'Precision': round(precision_score(y_true, y_pred), 3),\n",
    "        'Recall': round(recall_score(y_true, y_pred), 3),\n",
    "        'F1-Score': round(f1_score(y_true, y_pred), 3)\n",
    "    }\n",
    "\n",
    "# Computando a validacao do modelo\n",
    "dt_report = calculate_metrics(y, dt_y_pred_cv, 'Decision Tree')\n",
    "rf_report = calculate_metrics(y, rf_y_pred_cv, 'Random Forest')\n",
    "gb_report = calculate_metrics(y, gb_y_pred_cv, 'Naive Bayes')\n",
    "ab_report = calculate_metrics(y, ab_y_pred_cv, 'Ada Boost')\n",
    "kn_report = calculate_metrics(y, kn_y_pred_cv, 'K-nearest')\n",
    "nb_report = calculate_metrics(y, nb_y_pred_cv, 'Gradient Boosting')\n",
    "lr_report = calculate_metrics(y, lr_y_pred_cv, 'Logistic Regression')\n",
    "sv_report = calculate_metrics(y, sv_y_pred_cv, 'SVM')\n",
    "nn_report = calculate_metrics(y, nn_y_pred_cv, 'Neural Network')\n",
    "\n",
    "reports = [dt_report, rf_report, gb_report, ab_report, kn_report, nb_report, lr_report, sv_report, nn_report]\n",
    "\n",
    "summary_df = pd.DataFrame(reports)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teste estatistico T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Crie um DataFrame vazio com as colunas de modelos\n",
    "model_names = list(acuracias_modelos.keys())\n",
    "comparison_df = pd.DataFrame(index=model_names, columns=model_names)\n",
    "\n",
    "# Preencha a diagonal principal com \"N/A\" ou algum outro valor apropriado\n",
    "comparison_df.values[[i for i in range(len(model_names))], [i for i in range(len(model_names))]] = \"N/A\"\n",
    "\n",
    "# Realize as comparações entre modelos e preencha a tabela com os valores P\n",
    "for i in range(len(model_names)):\n",
    "    for j in range(i + 1, len(model_names)):\n",
    "        t_statistic, p_value = stats.ttest_ind(acuracias_modelos[model_names[i]], acuracias_modelos[model_names[j]])\n",
    "        comparison_df.at[model_names[i], model_names[j]] = round(p_value, 3)\n",
    "        comparison_df.at[model_names[j], model_names[i]] = round(p_value, 3)\n",
    "\n",
    "# Visualize a tabela de comparação\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dentre os modelos propostos o metodo de **Gradient Boosting** foi aquele que teve o maior destaque juntamente do **Random Forest**. Para decidir entre qual é mais importante, seja Precisão ou Recall é importante se analisar o contexto no qual a empresa que está realizando a análise esta. \n",
    "\n",
    "Priorizar a **Precisão** esta correlacionado a ideia de diminuir o número de \"alarmes falsos\", o que é especialmente útil caso a empresa queira realizar ações caras ou significativas para intervir, como o oferecimento de bônus ou promoções para reter talentos.\n",
    "\n",
    "Priorizar o **Recall** se torna importante caso a empresa queira evitar perder funcionários devido a rotatividade por exemplo. Ter um recall baixo significa que muitos funcionários que realmente saíram não foram identificados pelo modelos, isso se torna crítico em setores ou funções onde a perda de talentos é muito custosa para a organização (por exemplo em casos altamente especializados)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretabilidade dos modelos gerados\n",
    "\n",
    "Modelos baseados em árvore podem ser usados para se analisar quais atributos tiveram mais importância no treinamento, a partir de uma análise de quais foram os nós raiz gerados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para Random Forest, por exemplo:\n",
    "importances_dt = grid_search_dt.best_estimator_.feature_importances_\n",
    "importances_rf = grid_search_rf.best_estimator_.feature_importances_\n",
    "importances_gb = grid_search_gb.best_estimator_.feature_importances_\n",
    "\n",
    "# Criando o DataFrame\n",
    "df_importances = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance_dt': importances_dt,\n",
    "    'importance_rf': importances_rf,\n",
    "    'importance_gb': importances_gb\n",
    "})\n",
    "\n",
    "# Crie uma coluna com o valor acumulado das importâncias\n",
    "df_importances['total_importance'] = df_importances['importance_dt'] + df_importances['importance_rf'] + df_importances['importance_gb']\n",
    "\n",
    "# Ordene o DataFrame com base na importância total, em ordem decrescente\n",
    "df_importances = df_importances.sort_values(by='total_importance', ascending=False)\n",
    "\n",
    "# Valores acumulados\n",
    "df_importances['importance_rf_acc'] = df_importances['importance_dt'] + df_importances['importance_rf']\n",
    "df_importances['importance_gb_acc'] = df_importances['importance_rf_acc'] + df_importances['importance_gb']\n",
    "\n",
    "# Configurar figura e eixos\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "colors = sns.color_palette(\"Pastel1\", n_colors=3)\n",
    "# Plotando importância para cada modelo\n",
    "plt.barh(df_importances['feature'], df_importances['importance_gb_acc'], color=colors[2], label='Gradient Boosting')\n",
    "plt.barh(df_importances['feature'], df_importances['importance_rf_acc'], color=colors[1], label='Random Forest')\n",
    "plt.barh(df_importances['feature'], df_importances['importance_dt'], color=colors[0], label='Decision Tree')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com estes dados podemos chegar a algumas conclusões importantes, sendo a principal o tempo de permanência do funcionário o principal fator de influência para saber se ele vai ficar na empresa ou não. Outras inferências podem ser feitas como a importância do tipo de industria sobre o tipo de cargo, o que indica que o clima e cultura empresarial são fortes candidatos para a análise de sobrevivência de funcionários. Por outro lado características pessoais de auto controle e ansiedade se mostraram também importantes, sendo estas provavelmente relacionadas a resiliência do indivíduo. É válido ressaltar que o meio de contratação pode acabar também indicando a existência de fatores culturais vindos daquela plataforma. E por fim que a idade é um fator crucial para saber a resiliência de um funcionário em uma vaga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
